{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "import pickle\n",
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "from matplotlib import pyplot as plt\n",
    "from nn_utils import MatrixFactorization\n",
    "from itertools import zip_longest,combinations\n",
    "import math\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import scipy.sparse as sp\n",
    "from recommendation_utils import get_ib_recommendations, get_pop_recommendations,get_mf_recommendations,get_ub_recommendations, get_nn_recommendations, get_random_recommendations,get_simrank_recommendations\n",
    "import gc\n",
    "import pycmf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'mimic'  #valid options are decagon,medical,movielens,mimic\n",
    "seed = 0 # By changing this value, we get differenet random splits for train, test sets\n",
    "train_test_split = 0.8 # This value shows the % of the train and data set\n",
    "min_items = 15\n",
    "top_n = 5 # This is used for creating precision - recall charts, 10 for mimic\n",
    "neighborhood = 80 # The neighborhood to use for using UBCF\n",
    "similarity_coefficient = 0.5\n",
    "train_batch =512 # Used for training the NN\n",
    "methods = ['Pathsim','IBCF','MF','Pathsim_ddi','IBCF_ddi','MF_ddi']\n",
    "ddi_datasets = ['mimic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('mimic.csv')\n",
    "columns = ['patient','drug']\n",
    "df = df[columns]\n",
    "columns = ['user','drug']\n",
    "df_ddi = pd.read_csv('mimic_ddi.csv')\n",
    "df_ddi = df_ddi[['Drug1','Drug2']]    \n",
    "df.columns = columns\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp_df = df.groupby('user')['drug'].unique()\n",
    "medicines=[]\n",
    "counter =0\n",
    "counter_temp= []\n",
    "for i in range(0, len(temp_df)):\n",
    "    if len(temp_df.iloc[i]) > 15:\n",
    "        counter_temp.append(counter)\n",
    "        user_medicine = f\"{temp_df.iloc[i][0]}\"\n",
    "        for drug in range(1, len(temp_df.iloc[i])):\n",
    "            user_medicine += f' {temp_df.iloc[i][drug]}'\n",
    "        medicines.append(user_medicine)\n",
    "    counter+=1\n",
    "all_users = list(df['user'].unique())\n",
    "users_to_keep = []\n",
    "for user in all_users:\n",
    "    if len(temp_df.loc[user])>min_items:\n",
    "        users_to_keep.append(user)\n",
    "train_df = df[df.user.isin(users_to_keep)]\n",
    "train_df =  train_df.drop_duplicates()\n",
    "train_df = train_df.sort_values(by='user')\n",
    "train_df = train_df.reset_index()\n",
    "train_df = train_df.drop('index',axis=1)\n",
    "# records=dill.load(open('records_final_for_my.pkl','rb'))\n",
    "# data_to_keep =[records[x] for x in users_to_keep]\n",
    "# len(data_to_keep)\n",
    "# #     del records\n",
    "# diagnosis = [row[0] for row in data_to_keep]\n",
    "# procedures = [row[1] for row in data_to_keep]\n",
    "# medicines = [row[2] for row in data_to_keep]\n",
    "# medicines_temp =[]\n",
    "# diagnosis_temp = []\n",
    "# procedure_temp = []\n",
    "# diagnosis_df = []\n",
    "\n",
    "# for row in range(0,len(medicines),1):\n",
    "#     medicine = medicines[row]\n",
    "#     if len(medicine.split(\" \"))>15:\n",
    "#         medicines_temp.append(medicine)\n",
    "# \n",
    "#         diagnosis_temp.append(diagnosis[row])\n",
    "#         user_diagnosis = diagnosis[row].split(\" \")\n",
    "#         for diagnosi in user_diagnosis:\n",
    "#             diagnosis_df.append((row,diagnosi))\n",
    "#         procedure_temp.append(procedures[row])\n",
    "#     counter+=1\n",
    "# diagnosis_df = pd.DataFrame(diagnosis_df)\n",
    "# diagnosis_df.columns= ['user','diagnosis']\n",
    "# medicines = medicines_temp\n",
    "# diagnosis= diagnosis_temp\n",
    "# procedures = procedure_temp\n",
    "# #     medicines_temp = []\n",
    "# #     for medicine in medicines:\n",
    "# #         if len(medicine.split(\" \"))>15:\n",
    "# #             medicines_temp.append(medicine)\n",
    "# #     medicines = medicines_temp\n",
    "# medicines_tokenizer = Tokenizer()\n",
    "# medicines_tokenizer.fit_on_texts(medicines)\n",
    "# medicines_maxlen = max([len(line.split(' ')) for line in medicines])\n",
    "# sequences = medicines_tokenizer.texts_to_sequences(medicines)\n",
    "\n",
    "# padded_medicines = pad_sequences(sequences, maxlen=medicines_maxlen, padding='post', truncating='post')\n",
    "# diagnosis_maxlen=max([len(line.split(' ')) for line in diagnosis])\n",
    "# procedure_maxlen=max([len(line.split(' ')) for line in procedures])\n",
    "# medicines_maxlen = max([len(line.split(' ')) for line in medicines])\n",
    "# # Creating a bag-of-words for Diagnosis and Procedures sets\n",
    "# # Each diagnosis/procedure code is taken from the EHR input file\n",
    "# diagnosis_tokenizer = Tokenizer()\n",
    "# diagnosis_tokenizer.fit_on_texts(diagnosis)\n",
    "# sequences = diagnosis_tokenizer.texts_to_sequences(diagnosis)\n",
    "\n",
    "# diagnosis_= pad_sequences(sequences, maxlen=diagnosis_maxlen, padding='post', truncating='post')\n",
    "\n",
    "# procedure_tokenizer=Tokenizer()\n",
    "# procedure_tokenizer.fit_on_texts(procedures)\n",
    "# sequences=procedure_tokenizer.texts_to_sequences(procedures)\n",
    "\n",
    "# procedure_=pad_sequences(sequences,maxlen=procedure_maxlen,padding='post',truncating='post')\n",
    "# diseases_to_train = []\n",
    "# medicines_to_train = []\n",
    "# for x,z, y in zip(diagnosis_, procedure_,padded_medicines):\n",
    "#     diseases_to_train.append([x,z])\n",
    "#     medicines_to_train.append([y])\n",
    "# diagnosis_icd =pd.read_csv('D_ICD_DIAGNOSES.csv')\n",
    "# diagnosis_icd= diagnosis_icd[['ICD9_CODE','SHORT_TITLE']]\n",
    "# procedures_icd =pd.read_csv('D_ICD_PROCEDURES.csv')\n",
    "# procedures_icd= procedures_icd[['ICD9_CODE','SHORT_TITLE']]\n",
    "atc_drug= pickle.load(open( \"atc_drug.pkl\", \"rb\" ))\n",
    "\n",
    "print(f'Users : {len(train_df.user.unique())}, Drugs : {len(train_df.drug.unique())}')\n",
    "print(train_df.shape)\n",
    "patient_ids= list(train_df['user'].unique())\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the correspondance between the drugs we have (codes from the dataset) and how we encode them in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_list = train_df.values.tolist()\n",
    "overlapping_drugs = []\n",
    "current_patient = data_list[0][0]\n",
    "current_drugs =[]\n",
    "for p_d in data_list:\n",
    "    if p_d[0] == current_patient:\n",
    "        current_drugs.append(p_d[1])\n",
    "    else:\n",
    "        overlapping_drugs.append(current_drugs)\n",
    "        current_drugs = []\n",
    "        current_drugs.append(p_d[1])\n",
    "        current_patient = p_d[0]\n",
    "drugs = []\n",
    "dict_drug = {}\n",
    "count= 0\n",
    "for drug_list in overlapping_drugs:\n",
    "    for drug in drug_list:\n",
    "        if drug not in dict_drug.keys():\n",
    "            dict_drug[drug] = count\n",
    "            drugs.append(count)\n",
    "            count+=1\n",
    "drugs = np.asarray(drugs)\n",
    "dict_drug['END'] = count\n",
    "medicineSet=[]\n",
    "new_medicines = []\n",
    "for row in medicines:\n",
    "    medicine_row = []\n",
    "    for item in row.split(' '):\n",
    "        medicine_row.append(item)\n",
    "        if item not in medicineSet:\n",
    "            medicineSet.append(item)\n",
    "    new_medicines.append(medicine_row)\n",
    "if dataset=='medical_2':\n",
    "    medicineSet1=[]\n",
    "    new_test_medicines = []\n",
    "    for row in test_medicines:\n",
    "        medicine_row = []\n",
    "        for item in row.split(' '):\n",
    "            medicine_row.append(item)\n",
    "            if item not in medicineSet:\n",
    "                medicineSet.append(item)\n",
    "        new_test_medicines.append(medicine_row)\n",
    "drug2id={drug:id for id,drug in enumerate(medicineSet)}\n",
    "drug2id['END']=len(drug2id)\n",
    "drug2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We iterate each user and create an array with each one corresponding index to interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_drug_list = []\n",
    "if dataset =='medical_2':\n",
    "    patient_test_drug_list=[]\n",
    "    for patient_drugs in new_test_medicines:\n",
    "\n",
    "        drugs_per_patient = []\n",
    "        for drug in patient_drugs: \n",
    "            drugs_per_patient.append(drug2id[drug])\n",
    "\n",
    "        patient_test_drug_list.append(drugs_per_patient)\n",
    "\n",
    "for patient_drugs in new_medicines:\n",
    "\n",
    "    drugs_per_patient = []\n",
    "    for drug in patient_drugs: \n",
    "        drugs_per_patient.append(drug2id[drug])\n",
    "\n",
    "    patient_drug_list.append(drugs_per_patient)\n",
    "patient_drug_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medicines = []\n",
    "# for i in range(0,len(patient_drug_list)):\n",
    "#     user_medicine = f'{patient_drug_list[i][0]}'\n",
    "#     for drug in range(1,len(patient_drug_list[i])):\n",
    "#         user_medicine+= f' {patient_drug_list[i][drug]}'\n",
    "#     medicines.append(user_medicine)\n",
    "# medicines_maxlen = max([len(line.split(' ')) for line in medicines])\n",
    "# medicines_tokenizer = Tokenizer()\n",
    "# medicines_tokenizer.fit_on_texts(medicines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the train split and split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "train_set=[]\n",
    "test_set =[]\n",
    "count = 0\n",
    "train_medicines = []\n",
    "\n",
    "for user_interests in patient_drug_list:\n",
    "    user_interests = np.asarray(user_interests)\n",
    "\n",
    "    indices = np.arange(len(user_interests))\n",
    "\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_end = int(train_test_split*len(user_interests))\n",
    "    train_indices = indices[:train_end]\n",
    "\n",
    "    train_indices = user_interests[train_indices]\n",
    "\n",
    "    test_indices = indices[train_end:]\n",
    "    test_indices = user_interests[test_indices]\n",
    "    train_interests = np.zeros(len(drugs))\n",
    "    test_interests = np.zeros(len(drugs))\n",
    "    print(train_indices)\n",
    "    train_interests[train_indices] = 1\n",
    "    train_medicines.append(train_indices)\n",
    "    train_set.append(train_interests)\n",
    "    test_interests[test_indices] = 1\n",
    "    test_set.append(test_interests)\n",
    "    print(f'Patient {count} has taken {len(user_interests)} drugs, {len(train_indices)} drugs are in the train set, {len(test_indices)} drugs are in his test set')\n",
    "    count+=1\n",
    "cancer_data = 0\n",
    "# rl_data_x, rl_data_y, drug2id = load_records( cancer_data =cancer_data,min_items= min_items, train_test_split = train_test_split,dataset = dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calcuate the user similarities and item similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if dataset =='mimic':\n",
    "#     diagnosis_dict = {}# We create a dict in order to have each diagnosis as an int\n",
    "#     count = 0\n",
    "#     for i in list(diagnosis_df['diagnosis'].unique()):\n",
    "\n",
    "#         diagnosis_dict[i] = count\n",
    "#         count+=1\n",
    "    \n",
    "#     diagnosis_train_set = np.zeros(shape = [len(diagnosis_df['user'].unique()),len(diagnosis_df['diagnosis'].unique())])\n",
    "#     #diagnosis_train_set is an array with rows as patients and columns as diseases\n",
    "#     diagnosis_list = diagnosis_df.values.tolist()\n",
    "#     for i in diagnosis_list:\n",
    "#         diagnosis_train_set[i[0]][diagnosis_dict[i[1]]] = 1#We set ones to the diseases of each patient\n",
    "#     diagnosis_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MatrixFactorization(\n",
    "    np.asarray(train_set).shape[0], np.asarray(train_set).shape[1], \n",
    "    n_factors=5)\n",
    "\n",
    "def user_user_cosine_similarity(train_set):\n",
    "    A_sparse = sparse.csr_matrix(train_set)\n",
    "    user_similarities = cosine_similarity(A_sparse)    \n",
    "    return user_similarities\n",
    "\n",
    "\n",
    "def drug_drug_cosine_similarity(train_set):\n",
    "    A_sparse = sparse.csr_matrix(train_set)\n",
    "    drug_similarities = cosine_similarity(A_sparse.T)    \n",
    "    return drug_similarities\n",
    "\n",
    "def mf_similarity(train_set):\n",
    "    from scipy.sparse.linalg import svds\n",
    "    A_sparse = sparse.csr_matrix(train_set)\n",
    "    U, sigma, Vt = svds(A_sparse, k = 50)\n",
    "    #drug_similarities = cosine_similarity(Vt.T)\n",
    "    sigma = np.diag(sigma)\n",
    "    user_predicted_ratings = np.dot(np.dot(U, sigma), Vt) \n",
    "    return user_predicted_ratings\n",
    "def mf_similarity_2(train_set,ddi=False,ddi_net = None):\n",
    "    from scipy.sparse.linalg import svds\n",
    "    if ddi:\n",
    "        model = pycmf.CMF(n_components=50)\n",
    "        U,V,Z =model.fit_transform(np.asarray(train_set),ddi_net)\n",
    "        drug_similarities = cosine_similarity(V)\n",
    "    else:\n",
    "        A_sparse = sparse.csr_matrix(train_set)\n",
    "\n",
    "        U, sigma, Vt = svds(A_sparse, k = 50)\n",
    "        drug_similarities = cosine_similarity(Vt.T)\n",
    "#     sigma = np.diag(sigma)\n",
    "#     user_predicted_ratings = np.dot(np.dot(U, sigma), Vt) \n",
    "    return drug_similarities\n",
    "\n",
    "def pathsim_drug_drug_similarity(train_set,ddi=0,ddi_net = None):\n",
    "    train_data = np.asarray(train_set)\n",
    "    patients= []\n",
    "    for i in range(0,train_data.shape[0]):\n",
    "        patients.append(f'P{i}')\n",
    "    drugs = []\n",
    "    for i in range(0,train_data.shape[1]):\n",
    "        drugs.append(f'D{i}')\n",
    "\n",
    "    g = nx.Graph()\n",
    "    g.add_nodes_from(patients, entity='P')\n",
    "    g.add_nodes_from(drugs, entity='D')\n",
    "    edges = np.where(train_data==1)\n",
    "    for i in range(0,len(edges[0])):\n",
    "        patient = 'P'+str(edges[0][i])\n",
    "        drug = 'D'+str(edges[1][i])\n",
    "        g.add_edge(patient,drug, edge_type='PD') \n",
    "    if ddi:\n",
    "        for ddi_pair in ddi_net:\n",
    "            d1 = 'D'+str(ddi_pair[0])\n",
    "            d2 = 'D'+str(ddi_pair[1])\n",
    "            g.add_edge(d1,d2,edge_type = 'DD')\n",
    "    path_len = 1\n",
    "    target_entity = 'D'\n",
    "    adj_entity = 'P'\n",
    "    G= g\n",
    "    target_nodes = [node for node, attr in G.nodes(data=True) if attr['entity'] == target_entity]\n",
    "    adj_nodes = [node for node, attr in G.nodes(data=True) if attr['entity'] == adj_entity]\n",
    "\n",
    "    # ------ Calculating Adjacency matrix between target and adjacency nodes\n",
    "    adj = defaultdict(list)\n",
    "\n",
    "    for n1 in target_nodes:\n",
    "        adj[n1] = defaultdict(list)\n",
    "        for n2 in adj_nodes:\n",
    "            adj[n1][n2] = len([path for path in nx.all_simple_paths(G, n1, n2, path_len)])\n",
    "\n",
    "    # ------ Calculating similarities between target nodes\n",
    "    sim = defaultdict(list)\n",
    "    n_paths = defaultdict(list)\n",
    "\n",
    "    for n1 in target_nodes:\n",
    "        sim[n1] = defaultdict(int) #list\n",
    "        n_paths[n1] = defaultdict(int)\n",
    "        for n2 in target_nodes:\n",
    "            sim[n1][n2] = -1\n",
    "            n_paths[n1][n2] = 0\n",
    "\n",
    "    for n1 in target_nodes:\n",
    "        for n2 in target_nodes:\n",
    "            if sim[n2][n1] == -1:\n",
    "                n_connecting_paths = 0\n",
    "                numerator = 0\n",
    "                denominator = 0\n",
    "                for n3 in adj_nodes:\n",
    "                    n_connecting_paths += adj[n1][n3] * adj[n2][n3]\n",
    "                    numerator += 2 * adj[n1][n3] * adj[n2][n3]\n",
    "                    denominator += adj[n1][n3] ** 2 + adj[n2][n3] ** 2\n",
    "                sim[n1][n2] = numerator / denominator if denominator != 0 else 0\n",
    "                sim[n2][n1] = sim[n1][n2]\n",
    "                n_paths[n1][n2] = n_connecting_paths\n",
    "                n_paths[n2][n1] = n_paths[n1][n2]\n",
    "    df = pd.DataFrame(data =sim)\n",
    "    if ddi:\n",
    "        target_entity = 'D'\n",
    "        adj_entity = 'D'\n",
    "        G= g\n",
    "        target_nodes = [node for node, attr in G.nodes(data=True) if attr['entity'] == target_entity]\n",
    "        adj_nodes = [node for node, attr in G.nodes(data=True) if attr['entity'] == adj_entity]\n",
    "\n",
    "        # ------ Calculating Adjacency matrix between target and adjacency nodes\n",
    "        adj = defaultdict(list)\n",
    "\n",
    "        for n1 in target_nodes:\n",
    "            adj[n1] = defaultdict(list)\n",
    "            for n2 in adj_nodes:\n",
    "                adj[n1][n2] = len([path for path in nx.all_simple_paths(G, n1, n2, path_len)])\n",
    "\n",
    "        # ------ Calculating similarities between target nodes\n",
    "        sim_ddi = defaultdict(list)\n",
    "        n_paths = defaultdict(list)\n",
    "\n",
    "        for n1 in target_nodes:\n",
    "            sim_ddi[n1] = defaultdict(int) #list\n",
    "            n_paths[n1] = defaultdict(int)\n",
    "            for n2 in target_nodes:\n",
    "                sim_ddi[n1][n2] = -1\n",
    "                n_paths[n1][n2] = 0\n",
    "\n",
    "        for n1 in target_nodes:\n",
    "            for n2 in target_nodes:\n",
    "                if sim_ddi[n2][n1] == -1:\n",
    "                    n_connecting_paths = 0\n",
    "                    numerator = 0\n",
    "                    denominator = 0\n",
    "                    for n3 in adj_nodes:\n",
    "                        n_connecting_paths += adj[n1][n3] * adj[n2][n3]\n",
    "                        numerator += 2 * adj[n1][n3] * adj[n2][n3]\n",
    "                        denominator += adj[n1][n3] ** 2 + adj[n2][n3] ** 2\n",
    "                    sim_ddi[n1][n2] = numerator / denominator if denominator != 0 else 0\n",
    "                    sim_ddi[n2][n1] = sim_ddi[n1][n2]\n",
    "                    n_paths[n1][n2] = n_connecting_paths\n",
    "                    n_paths[n2][n1] = n_paths[n1][n2]\n",
    "        df_ddi = pd.DataFrame(data = sim_ddi)\n",
    "        \n",
    "        \n",
    "        df = df.add(-1*df_ddi)\n",
    "    print(df)\n",
    "    n_paths_matrix = n_paths\n",
    "    return df.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_similarities = user_user_cosine_similarity(train_set) \n",
    "# if dataset=='mimic':\n",
    "#     user_disease_similarities = user_user_cosine_similarity(diagnosis_train_set)   \n",
    "#     user_disease_similarities = (similarity_coefficient* user_similarities) + ((1-similarity_coefficient)* user_disease_similarities)\n",
    "drug_similarities = drug_drug_cosine_similarity(train_set)             \n",
    "if 'MF' in methods:      \n",
    "    mf_similarities = mf_similarity(train_set)\n",
    "    print(mf_similarities.shape)\n",
    "if 'MF2' in methods:\n",
    "    mf_2_similarities = mf_similarity_2(train_set)\n",
    "    print(mf_2_similarities.shape,mf_2_similarities)\n",
    "if 'Pathsim' in methods:\n",
    "    pathsim_similarities = pathsim_drug_drug_similarity(train_set)\n",
    "    random_pathsim = np.random.rand(pathsim_similarities.shape[0],pathsim_similarities.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the neighborhood for the user based recommendation. We then calculate predictions based on the neighboor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drug_rank = {value:key for key, value in drug2id.items()}\n",
    "drug_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have an auxiliary network, like in Decagon or Medical, we can define an auxiliary similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddi_similarities = np.ones(shape=drug_similarities.shape)\n",
    "counter= 0\n",
    "for i in range(0,df_ddi.shape[0]):\n",
    "    drug_1 = df_ddi.iloc[i]['Drug1']\n",
    "    drug_2 = df_ddi.iloc[i]['Drug2']\n",
    "    if (drug_1 in dict_drug.keys()) and (drug_2 in dict_drug.keys()):\n",
    "        counter+=1\n",
    "        ddi_similarities[dict_drug[drug_1],dict_drug[drug_2]] = 0\n",
    "        ddi_similarities[dict_drug[drug_2],dict_drug[drug_1]] = 0\n",
    "print(counter)\n",
    "ddi_similarities= (ddi_similarities-ddi_similarities.mean())/ddi_similarities.std()\n",
    "min_p = ddi_similarities.min()\n",
    "max_p = ddi_similarities.max()\n",
    "ddi_similarities = (ddi_similarities-min_p)/(max_p-min_p)\n",
    "pair_list = []\n",
    "for pair in df_ddi[['Drug1','Drug2']].values:\n",
    "    if (pair[0] in dict_drug.keys()) and (pair[1] in dict_drug.keys()):\n",
    "        if [dict_drug[pair[0]],dict_drug[pair[1]]] not in pair_list:\n",
    "            pair_list.append([dict_drug[pair[0]],dict_drug[pair[1]]])\n",
    "if 'MF_ddi' in methods:\n",
    "    mf_ddi_similarities = mf_similarity_2(train_set,ddi=1,ddi_net = ddi_similarities)\n",
    "if 'Pathsim_ddi' in methods:\n",
    "    pathsim_ddi_similarities = pathsim_drug_drug_similarity(train_set,ddi=1,ddi_net = pair_list)\n",
    "    random_pathsim_ddi = np.random.rand(pathsim_ddi_similarities.shape[0],pathsim_ddi_similarities.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>PRECISION AND RECALL </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement some helping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall(recs,test_set,user_id,results_text_name,train_set):\n",
    "    if dataset =='mimic':\n",
    "        with open(results_text_name, 'a') as f:\n",
    "\n",
    "\n",
    "            if len(recs)==top_n:\n",
    "\n",
    "                prescriptions = np.asarray(test_set)[user_id,:]\n",
    "                train_prescriptions = np.asarray(train_set)[user_id,:]\n",
    "                prescriptions = np.where(prescriptions==1)[0]\n",
    "                train_prescriptions = np.where(train_prescriptions==1)[0]\n",
    "                train_medicines = []\n",
    "\n",
    "                if dataset =='mimic':\n",
    "                    patient_id = patient_ids[user_id]\n",
    "#                     print(patient_id, user_id, train_prescriptions)\n",
    "                    train_ndc = []\n",
    "                    for medicine in train_prescriptions:\n",
    "                        train_ndc.append(drug_rank[medicine])\n",
    "                        if drug_rank[medicine] in atc_drug.keys():\n",
    "                            train_medicines.append(atc_drug[drug_rank[medicine]])\n",
    "                        else:\n",
    "                            train_medicines.append(drug_rank[medicine])\n",
    "#                     current_diagnosi = diagnosis[user_id]\n",
    "#                     current_procedure = procedures[user_id]\n",
    "#         #             print(diagnosis_icd[diagnosis_icd['ICD9_CODE']==diagnosi]['SHORT_TITLE'].values[0])\n",
    "#         #             print(current_diagnosi)\n",
    "#                     diagnosi = []\n",
    "#                     procedure = []\n",
    "#                     for x in current_diagnosi.split(\" \"):\n",
    "#                         if len(diagnosis_icd[diagnosis_icd['ICD9_CODE']==x])>0:\n",
    "#                             diagnosi.append(diagnosis_icd[diagnosis_icd['ICD9_CODE']==x]['SHORT_TITLE'].values[0])\n",
    "\n",
    "#                     for x in current_procedure.split(\" \"):\n",
    "#                         if len(procedures_icd[procedures_icd['ICD9_CODE']==int(x)])>0:\n",
    "#                             procedure.append(procedures_icd[procedures_icd['ICD9_CODE']==int(x)]['SHORT_TITLE'].values[0])\n",
    "#                         else:\n",
    "#                             procedure.append(x)\n",
    "\n",
    "\n",
    "                    medicines_recommended = []\n",
    "                    correct_predictions = 0\n",
    "                    for rec in recs:\n",
    "                        if rec in prescriptions:\n",
    "                            if drug_rank[rec] in atc_drug.keys():\n",
    "                                medicines_recommended.append(f'** {atc_drug[drug_rank[rec]]} **')\n",
    "                            else:\n",
    "                                medicines_recommended.append(f'** {drug_rank[rec]} **')\n",
    "                            correct_predictions+=1\n",
    "                        else:\n",
    "                            if drug_rank[rec] in atc_drug.keys():\n",
    "                                medicines_recommended.append(atc_drug[drug_rank[rec]])\n",
    "                            else:\n",
    "                                medicines_recommended.append(drug_rank[rec])\n",
    "\n",
    "\n",
    "\n",
    "                #diagnosis\n",
    "\n",
    "\n",
    "                \n",
    "                drug_names = []\n",
    "                for drug in prescriptions:\n",
    "                    if drug_rank[drug] in atc_drug.keys():\n",
    "                        drug_names.append(atc_drug[drug_rank[drug]])\n",
    "                    else:\n",
    "                        drug_names.append(drug_rank[drug])\n",
    "                if method =='IBCF':\n",
    "                    if correct_predictions>2:\n",
    "                        print(f'Patient ID {user_id}',file=f)\n",
    "\n",
    "                        if dataset=='medical':\n",
    "                            print(f\"Patient's Diseases : {list(carc_df[carc_df['patient']==patient_id]['carcinoma'].values)}\",file=f)\n",
    "        #                     print(f'Farmako {rec} find in prescriptions {prescriptions}')\n",
    "#                         elif dataset=='mimic':\n",
    "#                             print(f\"Patient's Diseases : {diagnosi}/{procedure}\",file=f)\n",
    "                        print(f'Train Medicines : {train_medicines}',file=f)\n",
    "                        print(f'Predicted Medication {medicines_recommended}',file=f)\n",
    "                        print(f'Doctor’s actual prescribed Medication {drug_names}',file=f)    \n",
    "                else:\n",
    "                    print(f'Patient ID {user_id}',file=f)\n",
    "                    if dataset=='medical':\n",
    "                        print(f\"Patient's Diseases : {list(carc_df[carc_df['patient']==patient_id]['carcinoma'].values)}\",file=f)\n",
    "    #                     print(f'Farmako {rec} find in prescriptions {prescriptions}')\n",
    "#                     elif dataset=='mimic':\n",
    "#                         print(f\"Patient's Diseases : {diagnosi}/{procedure}\",file=f)\n",
    "                    print(f'Train Medicines : {train_medicines}',file=f)\n",
    "                    print(f'Train Medicines NDC : {train_ndc}',file=f)\n",
    "                    print(f'Predicted Medication {medicines_recommended}',file=f)\n",
    "                    print(f'Doctor’s actual prescribed Medication {drug_names}',file=f)    \n",
    "\n",
    "    correct_predictions = np.count_nonzero(np.asarray(test_set)[user_id][recs])\n",
    "    test_interests = np.count_nonzero(np.asarray(test_set)[user_id])\n",
    "    precision = correct_predictions/len(recs)\n",
    "    #print(f'Correct Predictions : {correct_predictions} - Test Interests : {test_interests} - Recommendations : {len(recs)}')\n",
    "    recall = correct_predictions/test_interests\n",
    "    return precision,recall    \n",
    "   \n",
    "def calculate_adj_matrix(train_set):\n",
    "    train_data = np.asarray(train_set)\n",
    "    patients= []\n",
    "    for i in range(0,train_data.shape[0]):\n",
    "        patients.append(f'P{i}')\n",
    "    drugs = []\n",
    "    for i in range(0,train_data.shape[1]):\n",
    "        drugs.append(f'D{i}')\n",
    "\n",
    "    g = nx.Graph()\n",
    "    g.add_nodes_from(patients, entity='P')\n",
    "    g.add_nodes_from(drugs, entity='D')\n",
    "    edges = np.where(train_data==1)\n",
    "    for i in range(0,len(edges[0])):\n",
    "        patient = 'P'+str(edges[0][i])\n",
    "        drug = 'D'+str(edges[1][i])\n",
    "        g.add_edge(patient,drug, edge_type='PD') \n",
    "\n",
    "    path_len = 1\n",
    "    target_entity = 'D'\n",
    "    adj_entity = 'P'\n",
    "    G= g\n",
    "    target_nodes = [node for node, attr in G.nodes(data=True) if attr['entity'] == target_entity]\n",
    "    adj_nodes = [node for node, attr in G.nodes(data=True) if attr['entity'] == adj_entity]\n",
    "\n",
    "    # ------ Calculating Adjacency matrix between target and adjacency nodes\n",
    "    adj = defaultdict(list)\n",
    "\n",
    "    for n1 in target_nodes:\n",
    "        adj[n1] = defaultdict(list)\n",
    "        for n2 in adj_nodes:\n",
    "            adj[n1][n2] = len([path for path in nx.all_simple_paths(G, n1, n2, path_len)])\n",
    "    return adj\n",
    "def calculate_adj_matrix_ddi(train_set,ddi_net):\n",
    "    train_data = np.asarray(train_set)\n",
    "    patients= []\n",
    "    for i in range(0,train_data.shape[0]):\n",
    "        patients.append(f'P{i}')\n",
    "    drugs = []\n",
    "    for i in range(0,train_data.shape[1]):\n",
    "        drugs.append(f'D{i}')\n",
    "    \n",
    "    g = nx.Graph()\n",
    "    g.add_nodes_from(patients, entity='P')\n",
    "    g.add_nodes_from(drugs, entity='D')\n",
    "    edges = np.where(train_data==1)\n",
    "    for i in range(0,len(edges[0])):\n",
    "        patient = 'P'+str(edges[0][i])\n",
    "        drug = 'D'+str(edges[1][i])\n",
    "        g.add_edge(patient,drug, edge_type='PD') \n",
    "    for ddi_pair in ddi_net:\n",
    "            d1 = 'D'+str(ddi_pair[0])\n",
    "            d2 = 'D'+str(ddi_pair[1])\n",
    "            g.add_edge(d1,d2,edge_type = 'DD')\n",
    "    path_len = 1\n",
    "    target_entity = 'D'\n",
    "    adj_entity = 'D'\n",
    "    G= g\n",
    "    target_nodes = [node for node, attr in G.nodes(data=True) if attr['entity'] == target_entity]\n",
    "    adj_nodes = [node for node, attr in G.nodes(data=True) if attr['entity'] == adj_entity]\n",
    "\n",
    "    # ------ Calculating Adjacency matrix between target and adjacency nodes\n",
    "    adj = defaultdict(list)\n",
    "\n",
    "    for n1 in target_nodes:\n",
    "        adj[n1] = defaultdict(list)\n",
    "        for n2 in adj_nodes:\n",
    "            adj[n1][n2] = len([path for path in nx.all_simple_paths(G, n1, n2, path_len)])\n",
    "    return adj\n",
    "def calculate_drugs_metapaths(drugs,adj,patient_dict):\n",
    "    \n",
    "    \n",
    "    adj_list = []\n",
    "    #patient_drugs = list(np.where(train_data[0]==1)[0])\n",
    "    total_drug_metapaths= 0\n",
    "\n",
    "    for current_drug in drugs:\n",
    "\n",
    "        for p,number in adj[f'D{current_drug}'].items():\n",
    "                if number ==1:\n",
    "                    adj_list.append(p)\n",
    "\n",
    "        for patient in adj_list:\n",
    "            total_drug_metapaths += patient_dict[patient]\n",
    "    return total_drug_metapaths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will calculate the precision and recall for a range of patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matrix = calculate_adj_matrix(train_set)\n",
    "adj_ddi = calculate_adj_matrix_ddi(train_set,pair_list)\n",
    "patient_dict ={}\n",
    "for drug,patient in adj_matrix.items():\n",
    "    for p,number in patient.items():\n",
    "        \n",
    "        if p in patient_dict.keys():\n",
    "            1\n",
    "        else:\n",
    "            patient_dict[p]=0\n",
    "        \n",
    "        if number==1:\n",
    "            patient_dict[p]+=1\n",
    "drug_dict_ddi ={}\n",
    "for drug,patient in adj_ddi.items():\n",
    "    for p,number in patient.items():\n",
    "        \n",
    "        if p in drug_dict_ddi.keys():\n",
    "            1\n",
    "        else:\n",
    "            drug_dict_ddi[p]=0\n",
    "        \n",
    "        if number==1:\n",
    "            drug_dict_ddi[p]+=1\n",
    "recommended_items = []\n",
    "net.eval()\n",
    "train_data =np.asarray(train_set)\n",
    "precisions = []\n",
    "recalls =[]\n",
    "explainabilities = []\n",
    "ddi_metric = []\n",
    "results_text_name = 'results.txt'\n",
    "for method in methods:\n",
    "        with open(results_text_name, 'a') as f:\n",
    "            print(method,file=f)\n",
    "        method_precisions=[]\n",
    "        method_recalls=[]\n",
    "        method_ddi = []\n",
    "        method_explainability = []\n",
    "        print(f'Training started at {datetime.now()}')\n",
    "        for recommendations in range (1,top_n+1):\n",
    "            normal_ddi = 1\n",
    "            temp_precision = []\n",
    "\n",
    "            temp_recall = []\n",
    "            temp_explainability = []\n",
    "\n",
    "            print(f'Starting recommendations for method {method} for top - :{recommendations}')\n",
    "\n",
    "            for user_id in range (0,np.asarray(train_set).shape[0]):\n",
    "                total_metapaths = calculate_drugs_metapaths(drugs=list(np.where(np.asarray(train_set)[0]==1)[0]),adj=adj_matrix,patient_dict=patient_dict)\n",
    "                \n",
    "                if method =='IBCF':\n",
    "                    recs = get_ib_recommendations(user_id, train_data,neighborhood,recommendations,drug_similarities)\n",
    "                    if dataset in ddi_datasets:\n",
    "                        for pair in combinations(recs,2):\n",
    "\n",
    "                                if (list(pair) in pair_list):\n",
    "                                    normal_ddi+=1\n",
    "                elif method =='Pathsim':\n",
    "                    recs = get_ib_recommendations(user_id, train_data,neighborhood,recommendations,0.8* pathsim_similarities + 0.2* random_pathsim)\n",
    "                    if dataset in ddi_datasets:\n",
    "                        for pair in combinations(recs,2):\n",
    "\n",
    "                                if (list(pair) in pair_list):\n",
    "                                    normal_ddi+=1\n",
    "                elif method =='Pathsim_ddi':\n",
    "                    recs = get_ib_recommendations(user_id, train_data,neighborhood,recommendations,0.8* pathsim_ddi_similarities + 0.2 * random_pathsim_ddi)\n",
    "                    if dataset in ddi_datasets:\n",
    "                        for pair in combinations(recs,2):\n",
    "\n",
    "                                if (list(pair) in pair_list):\n",
    "                                    normal_ddi+=1\n",
    "                elif method == 'MF':\n",
    "                    recs = get_mf_recommendations(user_id, train_data,neighborhood,recommendations,mf_similarities)\n",
    "                    if dataset in ddi_datasets:\n",
    "                        for pair in combinations(recs,2):\n",
    "\n",
    "                            if (list(pair) in pair_list):\n",
    "                                normal_ddi+=1\n",
    "                elif method =='MF2':\n",
    "                    recs = get_ib_recommendations(user_id, train_data,neighborhood,recommendations,mf_2_similarities)\n",
    "                    if dataset in ddi_datasets:\n",
    "                        for pair in combinations(recs,2):\n",
    "\n",
    "                                if (list(pair) in pair_list):\n",
    "                                    normal_ddi+=1\n",
    "                elif method =='MF_ddi':\n",
    "                    recs = get_ib_recommendations(user_id, train_data,neighborhood,recommendations,mf_ddi_similarities)\n",
    "                    if dataset in ddi_datasets:\n",
    "                        for pair in combinations(recs,2):\n",
    "\n",
    "                                if (list(pair) in pair_list):\n",
    "                                    normal_ddi+=1\n",
    "                elif method == 'POP':\n",
    "                    recs = get_pop_recommendations(user_id, train_data,neighborhood,recommendations,0)\n",
    "                    if dataset in ddi_datasets:\n",
    "                        for pair in combinations(recs,2):\n",
    "\n",
    "                            if (list(pair) in pair_list):\n",
    "                                normal_ddi+=1\n",
    "                elif method == 'IBCF_ddi':\n",
    "                    normalized_drug_similarities= (drug_similarities-drug_similarities.mean())/drug_similarities.std()\n",
    "                    min_p = normalized_drug_similarities.min()\n",
    "                    max_p = normalized_drug_similarities.max()\n",
    "                    normalized_drug_similarities = (normalized_drug_similarities-min_p)/(max_p-min_p)\n",
    "                    recs = get_ib_recommendations(user_id, train_data,neighborhood,recommendations,\n",
    "                                                  drug_similarities+ddi_similarities)\n",
    "                    if dataset in ddi_datasets:\n",
    "                        for pair in combinations(recs,2):\n",
    "\n",
    "                            if (list(pair) in pair_list):\n",
    "\n",
    "                                normal_ddi+=1\n",
    "                \n",
    "                elif method =='random':\n",
    "                    recs = get_random_recommendations(user_id,train_data,recommendations)\n",
    "                    if dataset in ddi_datasets:\n",
    "                        for pair in combinations(recs,2):\n",
    "\n",
    "                            if (list(pair) in pair_list):\n",
    "                                normal_ddi+=1\n",
    "\n",
    "\n",
    "                else:\n",
    "                        recs = get_nn_recommendations(user_id, train_data,neighborhood,recommendations,0,net=net)\n",
    "                        if dataset in ddi_datasets:\n",
    "                            for pair in combinations(recs,2):\n",
    "\n",
    "                                if (list(pair) in pair_list):\n",
    "                                    normal_ddi+=1\n",
    "                recs_metapaths = calculate_drugs_metapaths(drugs=recs,adj=adj_matrix,patient_dict=patient_dict)\n",
    "                if method=='Pathsim':\n",
    "                    drug_drug_metapath = 0\n",
    "                    for rec in recs:\n",
    "                        drug_drug_metapath +=drug_dict_ddi[f'D{rec}']\n",
    "\n",
    "                    recs_metapaths +=drug_drug_metapath\n",
    "                explainability = 10* min(recs_metapaths,total_metapaths)/total_metapaths\n",
    "                precision,recall = calculate_precision_recall(recs,test_set,user_id,results_text_name=results_text_name,train_set = train_set)\n",
    "                temp_precision.append(precision)\n",
    "                temp_recall.append(recall)\n",
    "                temp_explainability.append(explainability)\n",
    "            method_precisions.append(np.asarray(temp_precision).mean())\n",
    "            method_recalls.append(np.asarray(temp_recall).mean())\n",
    "            method_explainability.append(np.asarray(temp_explainability).mean())\n",
    "            method_ddi.append(normal_ddi)\n",
    "        ddi_metric.append(method_ddi)\n",
    "        precisions.append(method_precisions)\n",
    "        recalls.append(method_recalls)\n",
    "        explainabilities.append(method_explainability)\n",
    "        print(f'Training ended at {datetime.now()}')\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load DQN agent and produce results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z = zip(precisions,recalls)\n",
    "\n",
    "for precision,recall in z:\n",
    "    plt.plot(recall,precision)\n",
    "\n",
    "\n",
    "# plt.plot(ub_recalls,ub_precisions, ib_recalls,ib_precisions,pop_recalls,pop_precisions)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend(methods)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for method in methods:\n",
    "    print(f'Method : {method}\\n Precision: {np.round(precisions[counter],4)} \\n Recall :   {np.round(recalls[counter],4)} \\n Explainability :    {np.round(explainabilities[counter],4)}')\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddi_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_ddi = []\n",
    "random_metric = ddi_metric[-1]\n",
    "for method_ddi in range(0,len(ddi_metric)-1):\n",
    "    ddi = ddi_metric[method_ddi]\n",
    "    method_metric = []\n",
    "    \n",
    "    for counter in range(0,len(ddi)):\n",
    "        \n",
    "        method_metric.append((ddi[counter]-random_metric[counter])/random_metric[counter]*100)\n",
    "    chart_ddi.append(method_metric)\n",
    "\n",
    "ddi_metric\n",
    "\n",
    "#chart_ddi.append(random_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_ddi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'IBCF_ddi' in methods:\n",
    "for i in range(0,len(chart_ddi)):\n",
    "    plt.plot(list(np.arange(1,top_n+1)),chart_ddi[i])\n",
    "plt.xlabel(\"Top-n\")\n",
    "plt.ylabel(\"DDI Change% over  Random\")\n",
    "plt.legend(methods[:-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_for_recommendations = np.asarray(train_set).shape[0]\n",
    "adv_ddi = []\n",
    "for metric in ddi_metric:\n",
    "    worst_ddi = []\n",
    "    counter = 2\n",
    "    for top_rec in metric[1:]:\n",
    "        all_possible_adv = users_for_recommendations*(counter*(counter-1))/2\n",
    "        worst_ddi.append(top_rec/all_possible_adv)\n",
    "        counter+=1\n",
    "    adv_ddi.append(worst_ddi)\n",
    "adv_ddi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(adv_ddi)):\n",
    "    plt.plot(list(np.arange(2,top_n+1)),adv_ddi[i])\n",
    "plt.xlabel(\"Top-n\")\n",
    "plt.ylabel(\"DDI Rate\")\n",
    "plt.legend(methods)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for method in methods:\n",
    "    print(f'Method : {method} has {adv_ddi[counter]} DDI Rate')\n",
    "    counter+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compnet",
   "language": "python",
   "name": "compnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
